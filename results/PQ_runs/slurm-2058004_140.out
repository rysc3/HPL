Using parameter file /users/rdscher/cs499/HPL/hopper/gcc-11/param_sweep/HPL_params.csv
Using template file /users/rdscher/cs499/HPL/hopper/gcc-11/param_sweep/HPL_template.dat
Read param line 140: 352,2,3
Read param BLOCK_SIZE: 352
Temp directory: /carc/scratch/users/rdscher/tmp.f8X7mNnH4M
Created temporary working directory: /carc/scratch/users/rdscher/tmp.f8X7mNnH4M/140
Now running in /carc/scratch/users/rdscher/tmp.f8X7mNnH4M/140
Running xhpl in /carc/scratch/users/rdscher/tmp.f8X7mNnH4M/140...
To request GPUs, add --gpus-per-node X or --gpus X, where X is the desired number of GPUs.
Job 2058145 running on hopper[058-059]
slurmstepd: error: Detected 1 oom-kill event(s) in StepId=2058145.0. Some of your processes may have been killed by the cgroup out-of-memory handler.
srun: error: hopper058: task 3: Out Of Memory
[1695262932.366610] [hopper058:60456:0]        mm_posix.c:206  UCX  ERROR open(file_name=/proc/60459/fd/14 flags=0x0) failed: No such file or directory
[hopper058:60456] *** An error occurred in MPI_Send
[hopper058:60456] *** reported by process [1738604544,0]
[hopper058:60456] *** on communicator MPI COMMUNICATOR 5 SPLIT FROM 3
[hopper058:60456] *** MPI_ERR_OTHER: known error not in list
[hopper058:60456] *** MPI_ERRORS_ARE_FATAL (processes in this communicator will now abort,
[hopper058:60456] ***    and potentially your MPI job)
slurmstepd: error: *** STEP 2058145.0 ON hopper058 CANCELLED AT 2023-09-20T20:22:12 ***
srun: Job step aborted: Waiting up to 32 seconds for job step to finish.
srun: error: hopper059: tasks 8-15: Killed
xhpl finished
Results: Gflops : Rate of execution for solving the linear system. The following parameter values will be used:
Results data: Gflops : Rate of execution for solving the linear system. The following parameter values will be used:
Results Gflops: used:
Writing input parameters and gflops to /users/rdscher/cs499/HPL/hopper/gcc-11/param_sweep/HPL_results.csv
Deleted /carc/scratch/users/rdscher/tmp.f8X7mNnH4M
slurmstepd: error: Detected 6 oom-kill event(s) in StepId=2058145.batch. Some of your processes may have been killed by the cgroup out-of-memory handler.
